/root/Codes/part3/rnn/data.py:358: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tensor_data = torch.load(f'{input_dir}/tensor_data.pt')
torch.Size([56, 64])
Parameters: 0.57M
Loading...
rnn/train.py:225: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(args.previous_model_path))
HybridMambaLMHeadModel(
  (backbone): HybridMixerModel(
    (embedding): Embedding(56, 64)
    (layers): ModuleList(
      (0-11): 12 x Block(
        (norm): RMSNorm()
        (mixer): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
    )
    (norm_f): RMSNorm()
    (attention): LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=64, out_features=512, bias=False)
        (k_proj): Linear(in_features=64, out_features=512, bias=False)
        (v_proj): Linear(in_features=64, out_features=512, bias=False)
        (o_proj): Linear(in_features=512, out_features=64, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=64, out_features=256, bias=False)
        (up_proj): Linear(in_features=64, out_features=256, bias=False)
        (down_proj): Linear(in_features=256, out_features=64, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((64,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-05)
    )
  )
  (lm_head): Linear(in_features=64, out_features=56, bias=False)
)
  0%|                                                                                                                                                     | 0/20 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.37it/s]
Initial | val loss: 13.960922622680664 | val acc: 0.0
start training
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.84it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.20it/s]
Step 0 | Train loss | 14.02527904510498 | Samples 0 | Train acc: 0.0 | Val loss: 13.960922622680664 | Val acc: 0.0 | learning rate: 5.128205128205128e-06
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.15it/s]
Step 20 | Train loss | 12.878107343401227 | Samples 5120 | Train acc: 0.0 | Val loss: 12.51827335357666 | Val acc: 0.0 | learning rate: 0.0001076923076923077
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.14it/s]
Step 40 | Train loss | 5.162723459848544 | Samples 10240 | Train acc: 0.0 | Val loss: 7.600135445594788 | Val acc: 0.0 | learning rate: 0.00019999996732975573
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.96it/s]
Step 59 | Train loss | 1.5877508560816447 | Samples 15104 | Train acc: 0.0 | Val loss: 3.716309356689453 | Val acc: 0.0 | learning rate: 0.00019999639812699283
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.14it/s]
Step 79 | Train loss | 0.893428099155426 | Samples 20224 | Train acc: 0.0 | Val loss: 3.501955711841583 | Val acc: 0.0 | learning rate: 0.00019998627064325686
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.22it/s]
Step 98 | Train loss | 0.658067686389191 | Samples 25088 | Train acc: 0.0 | Val loss: 3.362273597717285 | Val acc: 0.0 | learning rate: 0.0001999705982194192
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.17it/s]
Step 118 | Train loss | 0.5556855221756366 | Samples 30208 | Train acc: 0.0 | Val loss: 3.2475531339645385 | Val acc: 0.0 | learning rate: 0.00019994773216012516
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.16it/s]
Step 137 | Train loss | 0.43968357210573944 | Samples 35072 | Train acc: 0.0 | Val loss: 3.1435295581817626 | Val acc: 0.0 | learning rate: 0.000199919960409059
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.09it/s]
Step 157 | Train loss | 0.38988360573973835 | Samples 40192 | Train acc: 0.0 | Val loss: 3.010669839382172 | Val acc: 0.0 | learning rate: 0.00019988436145532682
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.83it/s]
Step 176 | Train loss | 0.3164108446088888 | Samples 45056 | Train acc: 0.0 | Val loss: 2.886941385269165 | Val acc: 0.0 | learning rate: 0.0001998444972769814
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.20it/s]
Step 196 | Train loss | 0.28761527380967505 | Samples 50176 | Train acc: 0.0 | Val loss: 2.788562262058258 | Val acc: 0.0 | learning rate: 0.0001997961742734451
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.15it/s]
Step 215 | Train loss | 0.24203465713395012 | Samples 55040 | Train acc: 0.0 | Val loss: 2.718622863292694 | Val acc: 0.0 | learning rate: 0.0001997442275721583
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.12it/s]
Step 235 | Train loss | 0.22784810955241575 | Samples 60160 | Train acc: 0.0 | Val loss: 2.657090973854065 | Val acc: 0.0 | learning rate: 0.0001996831925247685
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.66it/s]
Step 254 | Train loss | 0.19635195358126772 | Samples 65024 | Train acc: 0.0 | Val loss: 2.6069464325904845 | Val acc: 0.0 | learning rate: 0.00019961917620680617
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.27it/s]
Step 274 | Train loss | 0.18794300512834028 | Samples 70144 | Train acc: 0.0 | Val loss: 2.5647152423858643 | Val acc: 0.0 | learning rate: 0.00019954544427984713
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.20it/s]
Step 293 | Train loss | 0.16478642314469732 | Samples 75008 | Train acc: 0.0 | Val loss: 2.5288619875907896 | Val acc: 0.0 | learning rate: 0.00019946937425019648
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.11it/s]
Step 313 | Train loss | 0.16022093584583064 | Samples 80128 | Train acc: 0.0 | Val loss: 2.4976889967918394 | Val acc: 0.0 | learning rate: 0.00019938296376251864
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.04it/s]
Step 333 | Train loss | 0.14894996860070142 | Samples 85248 | Train acc: 0.0 | Val loss: 2.4719587326049806 | Val acc: 0.0 | learning rate: 0.0001992900595784356
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.20it/s]
Step 352 | Train loss | 0.1323738949157023 | Samples 90112 | Train acc: 0.0 | Val loss: 2.4478506565093996 | Val acc: 0.0 | learning rate: 0.000199195791341405
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.93it/s]
Step 372 | Train loss | 0.13095427582155283 | Samples 95232 | Train acc: 0.0 | Val loss: 2.4284976363182067 | Val acc: 0.0 | learning rate: 0.00019909024229533984
  5%|██████▋                                                                                                                             | 393/7812 [1:34:46<30:51:19, 14.97s/it]Traceback (most recent call last):
Step 391 | Train loss | 0.11735426406471097 | Samples 100096 | Train acc: 0.0 | Val loss: 2.4092111229896545 | Val acc: 0.0 | learning rate: 0.00019898397351988308
  File "rnn/train.py", line 242, in <module>
  File "rnn/train.py", line 239, in main
  File "rnn/train.py", line 93, in train
    loss.backward()
  File "/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/root/miniconda3/lib/python3.8/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
